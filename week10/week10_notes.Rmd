---
title: "SML 201 -- Week 10"
author: "John D. Storey"
date: "Spring 2016"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    keep_tex: true
geometry: right=2.5in
---

```{r my_opts, cache=FALSE, include=FALSE}
library(knitr)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1))  # smaller margin on top and right
})
opts_chunk$set(fig.align="center", fig.height=5.5, fig.width=6.75, collapse=TRUE, comment="", prompt=TRUE, small.mar=TRUE)
options(width=63)
library("ggplot2")
theme_set(theme_bw())
library("dplyr")
library("broom")
set.seed(201)
```


# Two Quantitative Variables

## Sample Correlation

Suppose we observe $n$ pairs of data $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$. Their sample correlation is

\begin{eqnarray}
r_{xy} & = & \frac{\sum_{i=1}^n (x_i - \overline{x}) (y_i - \overline{y})}{\sqrt{\sum_{i=1}^n (x_i - \overline{x})^2 \sum_{i=1}^n (y_i - \overline{y})^2}} \\
\ & = & \frac{\sum_{i=1}^n (x_i - \overline{x}) (y_i - \overline{y})}{(n-1) s_x s_y}
\end{eqnarray}

where $s_x$ and $s_y$ are the sample standard deviations of each measured variable.

## Hand Size Vs. Height

```{r, echo=FALSE, include=FALSE}
library("MASS")
data("survey", package="MASS")
```

```{r, warning=FALSE}
ggplot(data = survey, mapping=aes(x=Wr.Hnd, y=Height)) +
  geom_point() + geom_vline(xintercept=mean(survey$Wr.Hnd, na.rm=TRUE)) +
  geom_hline(yintercept=mean(survey$Height, na.rm=TRUE))
```

## HT of Correlation

```{r}
str(cor.test)
```

From the help file:

```
Usage

cor.test(x, ...)

## Default S3 method:
cor.test(x, y,
         alternative = c("two.sided", "less", "greater"),
         method = c("pearson", "kendall", "spearman"),
         exact = NULL, conf.level = 0.95, continuity = FALSE, 
         ...)

## S3 method for class 'formula'
cor.test(formula, data, subset, na.action, ...)
```

## HT of Correlation

```{r}
cor.test(x=survey$Wr.Hnd, y=survey$Height)
```

## HT By Hand

Compare the following to the above output of `cor.test()`.

```{r}
r <- cor(survey$Wr.Hnd, survey$Height, 
    use="pairwise.complete.obs")
df <- sum(complete.cases(survey[,c("Wr.Hnd", "Height")]))-2
# dplyr way to get df:
# df <- (survey %>% select(Wr.Hnd, Height) %>% 
#        na.omit() %>% nrow())-2 

tstat <- r/sqrt((1 - r^2)/df)
tstat

pvalue <- 2*pt(q=-abs(tstat), df=df)
pvalue
```

## Hand Sizes

```{r, warning=FALSE}
ggplot(data = survey) +
  geom_point(aes(x=Wr.Hnd, y=NW.Hnd))
```

## Correlation of Hand Sizes

```{r}
cor.test(x=survey$Wr.Hnd, y=survey$NW.Hnd)
```

## Davis Data

```{r, message=FALSE}
library("car")
data("Davis", package="car")
```

```{r}
htwt <- tbl_df(Davis)
htwt[12,c(2,3)] <- htwt[12,c(3,2)]
head(htwt)
```

## Height and Weight

```{r}
ggplot(htwt) + 
  geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) +
  scale_color_manual(values=c("red", "blue"))
```

## Correlation Test

```{r}
cor.test(x=htwt$height, y=htwt$weight)
```

## Correlation Test with Outlier

Recall we had to fix an error in the data, which we noticed as an outlier in the scatterplot.  Here is the effect of the outlier:

```{r}
cor.test(x=Davis$height, y=Davis$weight)
```

## Correlation Test with Outlier

Let's use the Spearman rank-based correlation:

```{r}
cor.test(x=Davis$height, y=Davis$weight, method="spearman")
```

## Correlation Among Females

```{r}
htwt %>% filter(sex=="F") %>%  
  cor.test(~ height + weight, data = .)
```

## Correlation Among Males

```{r}
htwt %>% filter(sex=="M") %>%  
  cor.test(~ height + weight, data = .)
```

Why are the stratified correlations lower?

# Least Squares Linear Regression

## Rationale

- It is often the case that we would like to build a model that explains the variation of one variable in terms of other variables.

- **Least squares linear regression** is one of the simplest and most useful modeling systems for doing so.

- It is simple to fit, it satisfies some optimality criteria, and it is straightforward to check assumptions on the data so that statistical inference can be performed.

## Setup

- Let's start with least squares linear regression of just two variables.  

- Suppose that we have observed $n$ pairs of data $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$.

- **Least squares linear regression** models variation of the **response variable** $y$ in terms of the **explanatory variable** $x$ in the form of $\beta_0 + \beta_1 x$, where $\beta_0$ and $\beta_1$ are chosen to satisfy a least squares optimization.

## Line that Minimizes the Squared Error

The least squares regression line is formed from the value of $\beta_0$ and $\beta_1$ that minimize:

$$\sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2.$$

For a given set of data, there is a unique solution to this minimization as long as there are at least two unique values among $x_1, x_2, \ldots, x_n$.  

Let $\hat{\beta_0}$ and $\hat{\beta_1}$ be the values that minimize this sum of squares.

## Least Squares Solution

These values are:

$$\hat{\beta}_1 = r_{xy} \frac{s_y}{s_x}$$

$$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

These values have a useful interpretation.

## Visualizing Least Squares Line

```{r, echo=FALSE}
set.seed(201)
x <- rnorm(50) + 20
y <- 10 + 2*x + rnorm(50)
f <- lm(y ~ x)

df <- data.frame(x=x, y=y, f=f$fitted.values, 
                 lower=pmin(y, f$fitted.values), 
                 upper=pmax(y, f$fitted.values))

ggplot(df) + 
  geom_line(aes(x, f), color="blue") + 
  geom_linerange(aes(x=x, ymin=lower, ymax=upper), color="red") +
  geom_point(aes(x,y)) +
  labs(x="x", y="y")
```

## Example: Height and Weight

```{r}
ggplot(data=htwt, mapping=aes(x=height, y=weight)) + 
  geom_point(size=2, alpha=0.5) +
  geom_smooth(method="lm", se=FALSE, formula=y~x)
```

## Calculate the Line Directly

```{r}
beta1 <- cor(htwt$height, htwt$weight) * 
               sd(htwt$weight) / sd(htwt$height)
beta1

beta0 <- mean(htwt$weight) - beta1 * mean(htwt$height)
beta0

yhat <- beta0 + beta1 * htwt$height
```

## Plot the Line

```{r}
df <- data.frame(htwt, yhat=yhat)
ggplot(data=df) + geom_point(aes(x=height, y=weight), size=2, alpha=0.5) +
  geom_line(aes(x=height, y=yhat), color="blue", size=1.2)
```

## Calculate the Line in R

The syntax for a model in R is 

```response variable ~ explanatory variables``` 

where the `explanatory variables` component can involve several types of terms.

```{r}
myfit <- lm(weight ~ height, data=htwt)
myfit
```

## What's Next?

- Why minimize the sum of squares?
- What is the output provided by R? 
- How do we access and interpret this output from R?
- What assumptions are required to use this machinery?
- How do we check these assumptions on data?
- How can we build more complex models?

# `lm` Object Class

## An `lm` Object is a List

```{r}
class(myfit)
is.list(myfit)
names(myfit)
```

## From the R Help

> `lm` returns an object of class "lm" or for multiple responses of class c("mlm", "lm").

> The functions `summary` and `anova` are used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by `lm`.

## Some of the List Items

These are some useful items to access from the `lm` object:

- `coefficients`: a named vector of coefficients
- `residuals`:	the residuals, that is response minus fitted values.
- `fitted.values`: the fitted mean values.
- `df.residual`: the residual degrees of freedom.
- `call`: the matched call.
- `model`: if requested (the default), the model frame used.

## `summary()`

```{r}
summary(myfit)
```

## `summary()` List Elements

```{r}
mysummary <- summary(myfit)
names(mysummary)
```

## Using `tidy()`

```{r}
library(broom)
tidy(myfit)
```

# More on the Underlying Model

## Probability Model

The typical probability model assumed for **ordinary least squares** is:

$Y_i = \beta_0 + \beta_1 X_i + E_i$

where ${\rm E}[E_i]=0$,  ${\rm Var}[E_i]= \sigma^2$, and $\rho_{E_i, E_j} = 0$ for all $i, j \in \{1, 2, \ldots, n\}$.

## Optimality

Fitting this linear model by least squares satisfies two types of optimality:

1.  [Gauss-Markov Theorem](https://en.wikipedia.org/wiki/Gaussâ€“Markov_theorem)
2.  [Maximum likelihood estimate](https://en.wikipedia.org/wiki/Ordinary_least_squares#Maximum_likelihood) when in addition $E_i \sim \mbox{Normal}(0,\sigma^2)$

Interested students can explore the above links.

## Observed Data, Fits, and Residuals

We observe data $(x_1, y_1), \ldots, (x_n, y_n)$. Note that we only observe the left hand side of the generative model $y_i = \beta_0 + \beta_1 x_i + e_i$.

We calculate fitted values and observed residuals:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$$

$$\hat{e}_i = y_i - \hat{y}_i$$

By construction, it is the case that $\sum_{i=1}^n \hat{e}_i = 0$.

##  Fitted Values Vs. Obs. Residuals

```{r, echo=FALSE, fig.width=10, fig.height=4}
par(mfrow=c(1,3))
set.seed(777)
n <- 60
x <- (1:n) * (20/n) + rnorm(n, sd=0.1)
y1 <- rnorm(n); y1 <- y1 - mean(y1)
y2 <- rnorm(n) + 0.07*(x-10.5)^2; y2 <- y2 - mean(y2); y2 <- y2/sd(y2)
y3 <- rnorm(n); y3 <- y3 - mean(y3)
y3 <- x^(1.5)*y3; y3 <- y3/sd(y3)
plot(x, y1, xlab=" ", ylab="Residuals", pch=20, cex=2,
     ylim=c(-2,2), cex.lab=1.2); abline(h=0)
plot(x, y2, xlab="Fitted Values", ylab=" ", pch=20, cex=2, 
     ylim=c(-2,2), cex.lab=1.5); abline(h=0)
plot(x, y3, xlab=" ", ylab=" ", pch=20, cex=2, 
     ylim=c(-2,2)); abline(h=0)
```

## Assumptions to Verify

The assumptions on the above linear model are really about the joint distribution of the residuals, which are not directly observed.  On data, we try to verify:

1. The fitted values and the residuals show no trends with respect to each other
1. The residuals are distributed approximately Normal$(0,\sigma^2)$
    - A constant variance is called [**homoscedasticity**](https://en.wikipedia.org/wiki/Homoscedasticity)
    - A non-constant variance is called [**heteroscedascity**](https://en.wikipedia.org/wiki/Heteroscedasticity)
1. There are no lurking variables

There are two plots we will use in this course to investigate the first two.

## Residual Distribution

```{r}
plot(myfit, which=1)
```

## Normal Residuals Check

```{r}
plot(myfit, which=2)
```

## Proportion of Variation Explained

The proportion of variance explained by the fitted model is called $R^2$ or $r^2$.  It is calculated by:

$$r^2 = \frac{s^2_{\hat{y}}}{s^2_{y}}$$

```{r}
summary(myfit)$r.squared

var(myfit$fitted.values)/var(htwt$weight)
```

# Categorial Explanatory Variables

## Example: Chicken Weights

```{r}
data("chickwts", package="datasets")
head(chickwts)
summary(chickwts$feed)
```

## Including Factor Variables in `lm()`

```{r}
chick_fit <- lm(weight ~ feed, data=chickwts)
summary(chick_fit)
```

## Plot the Fit

```{r}
plot(chickwts$feed, chickwts$weight, xlab="Feed", ylab="Weight")
points(chickwts$feed, chick_fit$fitted.values, col="blue", pch=20, cex=2)
```

## ANOVA (Version 1)

ANOVA (*analysis of variance*) was originally developed as a statistical model and method for comparing differences in mean values between various groups.

ANOVA quantifies and tests for differences in response variables with respect to factor variables.

In doing so, it also partitions the total variance to that due to within and between groups, where groups are defined by the factor variables.

## `anova()`

The classic ANOVA table:
```{r}
anova(chick_fit)
```

```{r}
n <- length(chick_fit$residuals) # n <- 71
(n-1)*var(chick_fit$fitted.values)
(n-1)*var(chick_fit$residuals)
(n-1)*var(chickwts$weight) # sum of above two quantities
(231129/5)/(195556/65) # F-statistic
```

## How It Works

```{r}
levels(chickwts$feed)
head(chickwts, n=3)
tail(chickwts, n=3)
x <- model.matrix(weight ~ feed, data=chickwts)
dim(x)
```

## Top of Design Matrix

```{r}
head(x)
```

## Bottom of Design Matrix

```{r}
tail(x)
```

## Model Fits

```{r}
chick_fit$fitted.values %>% round(digits=4) %>% unique()
```

```{r}
chickwts %>% group_by(feed) %>% summarize(mean(weight))
```

# Regression with Several Variables

## Weight Regressed on Height + Sex

```{r}
summary(lm(weight ~ height + sex, data=htwt))
```

## Ordinary Least Squares

Suppose we observe data $(x_{11}, x_{21}, \ldots, x_{d1}, y_1), \ldots, (x_{1n}, x_{2n}, \ldots, x_{dn}, y_n)$.  We have a response variable $y_i$ and $d$ explanatory variables $(x_{1i}, x_{2i}, \ldots, x_{di})$ per unit of observation.

Ordinary least squares models the variation of $y$ in terms of $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_d x_d$.

## OLS Solution

The estimates of $\beta_0, \beta_1, \ldots, \beta_d$ are found by identifying the values that minimize:

$$ \sum_{i=1}^n \left[ y_i - (\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_d x_{di}) \right]^2 $$

The solutions are expressed in terms of matrix algebra computations (see, e.g., [here](https://en.wikipedia.org/wiki/Ordinary_least_squares)).

## OLS in R

R implements OLS of multiple explanatory variables exactly the same as with a single explanatory variable, except we need to show the sum of all explanatory variables that we want to use.

```{r}
lm(weight ~ height + sex, data=htwt)
```

## A Twist on OLS

We can include a single variable but on two different scales:

```{r}
htwt <- htwt %>% mutate(height2 = height^2)
summary(lm(weight ~ height + height2, data=htwt))
```

## Interactions

It is possible to include products of explanatory variables, which is called an *interaction*.

```{r}
summary(lm(weight ~ height + sex + height:sex, data=htwt))
```

## More on Interactions

What happens when there is an interaction between a quantitative explanatory variable and a factor explanatory variable?  In the next plot, we show three models:

- Grey solid: `lm(weight ~ height, data=htwt)`
- Color dashed: `lm(weight ~ height + sex, data=htwt)`
- Color solid: `lm(weight ~ height + sex + height:sex, data=htwt)`

## Visualizing Three Different Models

```{r, echo=FALSE}
f1 <- lm(weight ~ height, data=htwt)
f2 <- lm(weight ~ height + sex, data=htwt)
f3 <- lm(weight ~ height + sex + height:sex, data=htwt)

fits <- data.frame(height=htwt$height, weight=htwt$weight, 
                   f1=f1$fitted.values, f2=f2$fitted.values,
                   f3=f3$fitted.values, sex=htwt$sex)

ggplot(data = fits) +
  geom_line(aes(x=height, y=f1), color="black", size=1.5, alpha=0.5) + 
  geom_line(aes(x=height, y=f2, color=sex), linetype=2, size=1.5, alpha=0.5) + 
  geom_line(aes(x=height, y=f3, color=sex), size=1.5, alpha=0.5) + 
  geom_point(aes(x=height, y=weight, color=sex)) +
  scale_color_manual(values = c("red", "blue")) + 
  labs(x="height", y="weight")
```

# Comparing Linear Models

## Example: Davis Data

Suppose we are considering the three following models:

```{r}
f1 <- lm(weight ~ height, data=htwt)
f2 <- lm(weight ~ height + sex, data=htwt)
f3 <- lm(weight ~ height + sex + height:sex, data=htwt)
```

How do we determine if the additional terms in models `f2` and `f3` are needed?

## ANOVA (Version 2)

A generalization of ANOVA exists that allows us to compare two nested models, quantifying their differences in terms of goodness of fit and performing a hypothesis test of whether this difference is statistically significant.  

A model is *nested* within another model if their difference is simply the absence of certain terms in the smaller model.  

The null hypothesis is that the additional terms have coefficients equal to zero, and the alternative hypothesis is that at least one coefficient is nonzero.

Both versions of ANOVA can be described in a single, elegant mathematical framework.

## Comparing Two Models <br> with `anova()`

This provides a comparison of the improvement in fit from model `f2` compared to model `f1`:
```{r}
anova(f1, f2)
```

## When There's a Single Variable Difference  

Compare above `anova(f1, f2)` p-value to that for the `sex` term from the `f2` model:
```{r}
tidy(f2)
```

## Calculating the F-statistic

```{r}
anova(f1, f2)
```

How the F-statistic is calculated:
```{r}
n <- nrow(htwt)
ss1 <- (n-1)*var(f1$residuals)
ss1
ss2 <- (n-1)*var(f2$residuals)
ss2
((ss1 - ss2)/anova(f1, f2)$Df[2])/(ss2/f2$df.residual)
```

## ANOVA on More Distant Models

We can compare models with multiple differences in terms:

```{r}
anova(f1, f3)
```

## Compare Multiple Models at Once

We can compare multiple models at once:

```{r}
anova(f1, f2, f3)
```

# Variable Transformations 

## Rationale

In order to obtain reliable model fits and inference on linear models, the model assumptions described earlier must be satisfied.  

Sometimes it is necessary to *transform* the response variable and/or some of the explanatory variables.

This process should involve data visualization and exploration.

## Power and Log Transformations

It is often useful to explore power and log transforms of the variables, e.g., $\log(y)$ or $y^\lambda$ for some $\lambda$ (and likewise $\log(x)$ or $x^\lambda$).

You can read more about the [Box-Cox family of power transformations](https://en.wikipedia.org/wiki/Power_transform).

## `Diamonds` Data

```{r, cache=TRUE}
data("diamonds", package="ggplot2")
head(diamonds)
```

## Nonlinear Relationship

```{r, cache=TRUE}
ggplot(data = diamonds) +
  geom_point(mapping=aes(x=carat, y=price, color=clarity), alpha=0.3)
```

## Regression with Nonlinear Relationship

```{r, cache=TRUE}
diam_fit <- lm(price ~ carat + clarity, data=diamonds)
anova(diam_fit)
```

## Residual Distribution

```{r, cache=TRUE}
plot(diam_fit, which=1)
```

## Normal Residuals Check

```{r, cache=TRUE}
plot(diam_fit, which=2)
```

## Log-Transformation

```{r, cache=TRUE}
ggplot(data = diamonds) +
  geom_point(aes(x=carat, y=price, color=clarity), alpha=0.3) +
  scale_y_log10(breaks=c(1000,5000,10000)) + 
  scale_x_log10(breaks=1:5)
```

## Regression on Log-Transformed Data

```{r, cache=TRUE}
diamonds <- mutate(diamonds, log_price = log(price, base=10), 
                   log_carat = log(carat, base=10))
ldiam_fit <- lm(log_price ~ log_carat + clarity, data=diamonds)
anova(ldiam_fit)
```

## Residual Distribution

```{r, cache=TRUE}
plot(ldiam_fit, which=1)
```

## Normal Residuals Check

```{r, cache=TRUE}
plot(ldiam_fit, which=2)
```

## Tree Pollen Study

```{r, echo=FALSE}
set.seed(123)
x <- rep(1:52, 10)
z <- rep(1:10, 52)
z <- as.integer(sort(z) + 2000)
y <- 50 + (3000/52)*(52-abs(20-x)) + rnorm(520, sd=200)
y <- pmax(1, y)
pollen_study <- tbl_df(data.frame(week=x, year=z, pollen=y))
```

Suppose that we have a study where tree pollen measurements are averaged every week, and these data are recorded for 10 years.

```{r}
pollen_study
```

## Tree Pollen Count by Week

```{r}
ggplot(pollen_study) + geom_point(aes(x=week, y=pollen))
```

## A Clever Transformation

We can see there is a linear relationship between `pollen` and `week` if we transform `week` to be number of weeks from the peak week.

```{r}
pollen_study <- pollen_study %>%  
                       mutate(week_new = abs(week-20))
```

Note that this is a very different transformation from taking a log or power transformation.

## `week` Transformed

```{r}
ggplot(pollen_study) + geom_point(aes(x=week_new, y=pollen))
```

# Extras

## License

<https://github.com/SML201/lectures/blob/master/LICENSE.md>

## Source Code

<https://github.com/SML201/lectures/tree/master/week10>

## Session Information

<section style="font-size: 0.75em;">
```{r}
sessionInfo()
```
</section>

